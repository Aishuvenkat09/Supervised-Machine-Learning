{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "#from __future__ import division\n",
    "import random\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>if_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  if_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = pd.read_csv(\"spambase.names\")\n",
    "\n",
    "\n",
    "#read data file data into dataframe\n",
    "list_column= ['word_freq_make','word_freq_address','word_freq_all','word_freq_3d','word_freq_our','word_freq_over','word_freq_remove','word_freq_internet','word_freq_order','word_freq_mail','word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses','word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your','word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george','word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data','word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm','word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project','word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(','char_freq_[','char_freq_!','char_freq_$','char_freq_#','capital_run_length_average','capital_run_length_longest','capital_run_length_total','if_spam']\n",
    "\n",
    "data = pd.read_csv(\"spambase.data\", names = list_column)\n",
    "data.head()\n",
    "\n",
    "\n",
    "#data.sample(frac = 0.394)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:57]\n",
    "Y = data['if_spam']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring the given condition: 0.39507246376811594\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "print(\"Ensuring the given condition:\", Y_train[Y_train==1].count()/len(Y_train))\n",
    "lgr = linear_model.LogisticRegression()\n",
    "lgr.fit(X_train,Y_train)\n",
    "\n",
    "y_pred = lgr.predict(X_test)\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[676,  25],\n",
       "       [ 58, 392]], dtype=int64)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 676\n",
      "False Positive: 58\n",
      "True Negative: 392\n",
      "False Negative: 25\n"
     ]
    }
   ],
   "source": [
    "tp,fn,fp,tn = confusion_matrix(Y_test, y_pred).ravel()\n",
    "\n",
    "print(\"True Positive:\",tp)\n",
    "print(\"False Positive:\",fp)\n",
    "print(\"True Negative:\",tn)\n",
    "print(\"False Negative:\",fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9278887923544744\n",
      "Error: 0.07211120764552559\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "print(\"Error:\" ,1- Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precesion 0.9209809264305178\n",
      "Recall 0.9643366619115549\n",
      "F1 score 0.9421602787456447\n"
     ]
    }
   ],
   "source": [
    "precesion = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "F1_score =  2* precesion *recall/(precesion+recall)\n",
    "print(\"Precesion\", precesion)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1 score\", F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[-0.09554938 -0.19227622  0.07566445  0.72470979  0.2856662   0.18300081\n",
      "   0.87149544  0.25480228  0.15339246  0.04435209 -0.06483383 -0.10648134\n",
      "  -0.00656088  0.02052661  0.28990717  0.94285368  0.36898037  0.03404817\n",
      "   0.1795526   0.37422615  0.23021244  0.27742507  0.76084584  0.15786092\n",
      "  -2.63201771 -1.2672251  -4.06271776  0.22180383 -0.9778588  -0.23369398\n",
      "  -0.16545071  0.12874899 -0.45804797 -1.02624285 -0.90119201  0.38348274\n",
      "  -0.16326097 -0.15898167 -0.37806418 -0.17588167 -1.64114644 -1.36745369\n",
      "  -0.24466596 -0.78162668 -0.97335243 -1.40634505 -0.16873209 -0.66275509\n",
      "  -0.35783348 -0.06881151 -0.28537876  0.27121816  1.24645131  0.95978406\n",
      "  -0.33305205  1.28403891  0.42166458]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Coefficients:\", lgr.coef_)\n",
    "\n",
    "c = lgr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>intercept</td>\n",
       "      <td>-2.851422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_make</td>\n",
       "      <td>-0.095549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_address</td>\n",
       "      <td>-0.192276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_all</td>\n",
       "      <td>0.075664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_3d</td>\n",
       "      <td>0.724710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_our</td>\n",
       "      <td>0.285666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_over</td>\n",
       "      <td>0.183001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>0.871495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_internet</td>\n",
       "      <td>0.254802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_order</td>\n",
       "      <td>0.153392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_mail</td>\n",
       "      <td>0.044352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_receive</td>\n",
       "      <td>-0.064834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_will</td>\n",
       "      <td>-0.106481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_people</td>\n",
       "      <td>-0.006561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_report</td>\n",
       "      <td>0.020527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_addresses</td>\n",
       "      <td>0.289907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_free</td>\n",
       "      <td>0.942854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_business</td>\n",
       "      <td>0.368980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_email</td>\n",
       "      <td>0.034048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_you</td>\n",
       "      <td>0.179553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_credit</td>\n",
       "      <td>0.374226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_your</td>\n",
       "      <td>0.230212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_font</td>\n",
       "      <td>0.277425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_000</td>\n",
       "      <td>0.760846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_money</td>\n",
       "      <td>0.157861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>-2.632018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hpl</td>\n",
       "      <td>-1.267225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_george</td>\n",
       "      <td>-4.062718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_650</td>\n",
       "      <td>0.221804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_lab</td>\n",
       "      <td>-0.977859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_labs</td>\n",
       "      <td>-0.233694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_telnet</td>\n",
       "      <td>-0.165451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_857</td>\n",
       "      <td>0.128749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_data</td>\n",
       "      <td>-0.458048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_415</td>\n",
       "      <td>-1.026243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_85</td>\n",
       "      <td>-0.901192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_technology</td>\n",
       "      <td>0.383483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_1999</td>\n",
       "      <td>-0.163261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_parts</td>\n",
       "      <td>-0.158982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_pm</td>\n",
       "      <td>-0.378064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_direct</td>\n",
       "      <td>-0.175882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_cs</td>\n",
       "      <td>-1.641146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_meeting</td>\n",
       "      <td>-1.367454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_original</td>\n",
       "      <td>-0.244666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_project</td>\n",
       "      <td>-0.781627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_re</td>\n",
       "      <td>-0.973352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_edu</td>\n",
       "      <td>-1.406345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_table</td>\n",
       "      <td>-0.168732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_conference</td>\n",
       "      <td>-0.662755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_;</td>\n",
       "      <td>-0.357833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_(</td>\n",
       "      <td>-0.068812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_[</td>\n",
       "      <td>-0.285379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>0.271218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>1.246451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_#</td>\n",
       "      <td>0.959784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>-0.333052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>1.284039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>0.421665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coefficients\n",
       "intercept                      -2.851422\n",
       "word_freq_make                 -0.095549\n",
       "word_freq_address              -0.192276\n",
       "word_freq_all                   0.075664\n",
       "word_freq_3d                    0.724710\n",
       "word_freq_our                   0.285666\n",
       "word_freq_over                  0.183001\n",
       "word_freq_remove                0.871495\n",
       "word_freq_internet              0.254802\n",
       "word_freq_order                 0.153392\n",
       "word_freq_mail                  0.044352\n",
       "word_freq_receive              -0.064834\n",
       "word_freq_will                 -0.106481\n",
       "word_freq_people               -0.006561\n",
       "word_freq_report                0.020527\n",
       "word_freq_addresses             0.289907\n",
       "word_freq_free                  0.942854\n",
       "word_freq_business              0.368980\n",
       "word_freq_email                 0.034048\n",
       "word_freq_you                   0.179553\n",
       "word_freq_credit                0.374226\n",
       "word_freq_your                  0.230212\n",
       "word_freq_font                  0.277425\n",
       "word_freq_000                   0.760846\n",
       "word_freq_money                 0.157861\n",
       "word_freq_hp                   -2.632018\n",
       "word_freq_hpl                  -1.267225\n",
       "word_freq_george               -4.062718\n",
       "word_freq_650                   0.221804\n",
       "word_freq_lab                  -0.977859\n",
       "word_freq_labs                 -0.233694\n",
       "word_freq_telnet               -0.165451\n",
       "word_freq_857                   0.128749\n",
       "word_freq_data                 -0.458048\n",
       "word_freq_415                  -1.026243\n",
       "word_freq_85                   -0.901192\n",
       "word_freq_technology            0.383483\n",
       "word_freq_1999                 -0.163261\n",
       "word_freq_parts                -0.158982\n",
       "word_freq_pm                   -0.378064\n",
       "word_freq_direct               -0.175882\n",
       "word_freq_cs                   -1.641146\n",
       "word_freq_meeting              -1.367454\n",
       "word_freq_original             -0.244666\n",
       "word_freq_project              -0.781627\n",
       "word_freq_re                   -0.973352\n",
       "word_freq_edu                  -1.406345\n",
       "word_freq_table                -0.168732\n",
       "word_freq_conference           -0.662755\n",
       "char_freq_;                    -0.357833\n",
       "char_freq_(                    -0.068812\n",
       "char_freq_[                    -0.285379\n",
       "char_freq_!                     0.271218\n",
       "char_freq_$                     1.246451\n",
       "char_freq_#                     0.959784\n",
       "capital_run_length_average     -0.333052\n",
       "capital_run_length_longest      1.284039\n",
       "capital_run_length_total        0.421665"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_input=pd.DataFrame(np.append(lgr.intercept_ , lgr.coef_[0]),\n",
    "                        index=np.append('intercept',data.iloc[:1,0:-1].T.index), columns=['coefficients'])\n",
    "\n",
    "coef_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_sort = coef_input.sort_values('coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26th feauture is highly correlated. i.e, word_freq_george"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>word_freq_george</td>\n",
       "      <td>-4.062718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>intercept</td>\n",
       "      <td>-2.851422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>-2.632018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_cs</td>\n",
       "      <td>-1.641146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_edu</td>\n",
       "      <td>-1.406345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_meeting</td>\n",
       "      <td>-1.367454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_hpl</td>\n",
       "      <td>-1.267225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_415</td>\n",
       "      <td>-1.026243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_lab</td>\n",
       "      <td>-0.977859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_re</td>\n",
       "      <td>-0.973352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_85</td>\n",
       "      <td>-0.901192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_project</td>\n",
       "      <td>-0.781627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_conference</td>\n",
       "      <td>-0.662755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_data</td>\n",
       "      <td>-0.458048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_pm</td>\n",
       "      <td>-0.378064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_;</td>\n",
       "      <td>-0.357833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>-0.333052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_[</td>\n",
       "      <td>-0.285379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_original</td>\n",
       "      <td>-0.244666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_labs</td>\n",
       "      <td>-0.233694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_address</td>\n",
       "      <td>-0.192276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_direct</td>\n",
       "      <td>-0.175882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_table</td>\n",
       "      <td>-0.168732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_telnet</td>\n",
       "      <td>-0.165451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_1999</td>\n",
       "      <td>-0.163261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_parts</td>\n",
       "      <td>-0.158982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_will</td>\n",
       "      <td>-0.106481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_make</td>\n",
       "      <td>-0.095549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_(</td>\n",
       "      <td>-0.068812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_receive</td>\n",
       "      <td>-0.064834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_people</td>\n",
       "      <td>-0.006561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_report</td>\n",
       "      <td>0.020527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_email</td>\n",
       "      <td>0.034048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_mail</td>\n",
       "      <td>0.044352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_all</td>\n",
       "      <td>0.075664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_857</td>\n",
       "      <td>0.128749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_order</td>\n",
       "      <td>0.153392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_money</td>\n",
       "      <td>0.157861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_you</td>\n",
       "      <td>0.179553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_over</td>\n",
       "      <td>0.183001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_650</td>\n",
       "      <td>0.221804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_your</td>\n",
       "      <td>0.230212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_internet</td>\n",
       "      <td>0.254802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>0.271218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_font</td>\n",
       "      <td>0.277425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_our</td>\n",
       "      <td>0.285666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_addresses</td>\n",
       "      <td>0.289907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_business</td>\n",
       "      <td>0.368980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_credit</td>\n",
       "      <td>0.374226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_technology</td>\n",
       "      <td>0.383483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>0.421665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_3d</td>\n",
       "      <td>0.724710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_000</td>\n",
       "      <td>0.760846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>0.871495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word_freq_free</td>\n",
       "      <td>0.942854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_#</td>\n",
       "      <td>0.959784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>1.246451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>1.284039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coefficients\n",
       "word_freq_george               -4.062718\n",
       "intercept                      -2.851422\n",
       "word_freq_hp                   -2.632018\n",
       "word_freq_cs                   -1.641146\n",
       "word_freq_edu                  -1.406345\n",
       "word_freq_meeting              -1.367454\n",
       "word_freq_hpl                  -1.267225\n",
       "word_freq_415                  -1.026243\n",
       "word_freq_lab                  -0.977859\n",
       "word_freq_re                   -0.973352\n",
       "word_freq_85                   -0.901192\n",
       "word_freq_project              -0.781627\n",
       "word_freq_conference           -0.662755\n",
       "word_freq_data                 -0.458048\n",
       "word_freq_pm                   -0.378064\n",
       "char_freq_;                    -0.357833\n",
       "capital_run_length_average     -0.333052\n",
       "char_freq_[                    -0.285379\n",
       "word_freq_original             -0.244666\n",
       "word_freq_labs                 -0.233694\n",
       "word_freq_address              -0.192276\n",
       "word_freq_direct               -0.175882\n",
       "word_freq_table                -0.168732\n",
       "word_freq_telnet               -0.165451\n",
       "word_freq_1999                 -0.163261\n",
       "word_freq_parts                -0.158982\n",
       "word_freq_will                 -0.106481\n",
       "word_freq_make                 -0.095549\n",
       "char_freq_(                    -0.068812\n",
       "word_freq_receive              -0.064834\n",
       "word_freq_people               -0.006561\n",
       "word_freq_report                0.020527\n",
       "word_freq_email                 0.034048\n",
       "word_freq_mail                  0.044352\n",
       "word_freq_all                   0.075664\n",
       "word_freq_857                   0.128749\n",
       "word_freq_order                 0.153392\n",
       "word_freq_money                 0.157861\n",
       "word_freq_you                   0.179553\n",
       "word_freq_over                  0.183001\n",
       "word_freq_650                   0.221804\n",
       "word_freq_your                  0.230212\n",
       "word_freq_internet              0.254802\n",
       "char_freq_!                     0.271218\n",
       "word_freq_font                  0.277425\n",
       "word_freq_our                   0.285666\n",
       "word_freq_addresses             0.289907\n",
       "word_freq_business              0.368980\n",
       "word_freq_credit                0.374226\n",
       "word_freq_technology            0.383483\n",
       "capital_run_length_total        0.421665\n",
       "word_freq_3d                    0.724710\n",
       "word_freq_000                   0.760846\n",
       "word_freq_remove                0.871495\n",
       "word_freq_free                  0.942854\n",
       "char_freq_#                     0.959784\n",
       "char_freq_$                     1.246451\n",
       "capital_run_length_longest      1.284039"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_freq_george,\n",
    "word_freq_hp,\n",
    "word_freq_cs,\n",
    "word_freq_edu,\n",
    "word_freq_meeting,\n",
    "word_freq_hpl,\n",
    "word_freq_415,\n",
    "word_freq_lab,\n",
    "word_freq_re,\n",
    "word_freq_85,,\n",
    "word_freq_project,\n",
    "word_freq_conference,\n",
    "word_freq_data,\n",
    "word_freq_pm,\n",
    "char_freq_;\n",
    "capital_run_length_average,\n",
    "char_freq_[\n",
    "\n",
    "word_freq_labs,\n",
    "word_freq_address,\n",
    "word_freq_direct,\n",
    "word_freq_table,\n",
    "word_freq_telnet,\n",
    "word_freq_1999,\n",
    "word_freq_parts,\n",
    "word_freq_will,\n",
    "word_freq_make,\n",
    "char_freq_(,\n",
    "word_freq_receive,\n",
    "word_freq_people  are negatively correlated and rest are positively correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########For i =0.25##########\n",
      "Confusion Matrix: [[613  88]\n",
      " [ 26 424]]\n",
      "Accuracy 0.9009556907037359\n",
      "Error 0.09904430929626407\n",
      "Precesion 0.828125\n",
      "Recall 0.9422222222222222\n",
      "F1 score 0.8814968814968815\n",
      "###########For i =0.5##########\n",
      "Confusion Matrix: [[676  25]\n",
      " [ 58 392]]\n",
      "Accuracy 0.9278887923544744\n",
      "Error 0.07211120764552559\n",
      "Precesion 0.9400479616306955\n",
      "Recall 0.8711111111111111\n",
      "F1 score 0.9042675893886967\n",
      "###########For i =0.75##########\n",
      "Confusion Matrix: [[694   7]\n",
      " [111 339]]\n",
      "Accuracy 0.89748045178106\n",
      "Error 0.10251954821894005\n",
      "Precesion 0.9797687861271677\n",
      "Recall 0.7533333333333333\n",
      "F1 score 0.8517587939698493\n",
      "###########For i =0.9##########\n",
      "Confusion Matrix: [[697   4]\n",
      " [193 257]]\n",
      "Accuracy 0.8288444830582102\n",
      "Error 0.17115551694178976\n",
      "Precesion 0.9846743295019157\n",
      "Recall 0.5711111111111111\n",
      "F1 score 0.7229254571026724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score,confusion_matrix,roc_auc_score, recall_score, roc_curve, precision_score\n",
    "pred_df = pd.DataFrame(lgr.predict_proba(X_test))\n",
    "threshold_list = [0.25,0.5,0.75,0.9]\n",
    "for i in threshold_list:\n",
    "    print(\"###########For i ={}##########\".format(i))\n",
    "    Y_pred = pred_df.applymap(lambda x: 1 if x>i else 0)\n",
    "    conf = confusion_matrix(Y_test, Y_pred[1])\n",
    "    print(\"Confusion Matrix:\", conf)\n",
    "    print(\"Accuracy\", accuracy_score(Y_test, Y_pred[1]))\n",
    "    print(\"Error\", 1- accuracy_score(Y_test, Y_pred[1]))\n",
    "    print(\"Precesion\", precision_score(Y_test, Y_pred[1]))\n",
    "    print(\"Recall\", recall_score(Y_test, Y_pred[1]))\n",
    "    print(\"F1 score\", f1_score(Y_test, Y_pred[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As threshold increases, Accuracy increases and decreases after a point\n",
    "precesion increasing\n",
    "recall decreases\n",
    "f1 score fluctuates as it depends on precesion and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "def net_input(theta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "def H(theta, x):\n",
    "    return sigmoid(net_input(theta, x))\n",
    "\n",
    "\n",
    "theta = np.zeros(len(X_test.columns))\n",
    "#print(theta)\n",
    "def GD(x,y,theta, Learning_rate, iterations):\n",
    "    cost_list = []\n",
    "    theta_list = []\n",
    "    pred_list =[]\n",
    "    \n",
    "    \n",
    "    for i in range(iterations):\n",
    "       \n",
    "        m = len(x)\n",
    "        predict = sigmoid(np.dot(x, theta))\n",
    "        #print(predict)\n",
    "        total_cost = - ((y * np.log(H(theta,x)) + (1-y) * np.log(1- H(theta, x)))).mean()\n",
    "        #print(total_cost)\n",
    "        cost_list.append(total_cost)\n",
    "        d_theta = (1/len(x)) * np.dot(x.T, sigmoid(net_input(theta,x))-y)\n",
    "        \n",
    "        theta = theta - Learning_rate * d_theta\n",
    "        \n",
    "        theta_list.append(theta)\n",
    "        pred_list.append(predict)\n",
    "        \n",
    "        \n",
    "        #print(\"##########################################################################################################\")\n",
    "        #print(\"cost {}, iteration {}\".format(total_cost, i))\n",
    "    print(total_cost)\n",
    "    \n",
    "    #print(theta1, theta0, cost)\n",
    "    return predict\n",
    "#GD(X_test,Y_test,theta, 0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "#######################################\n",
      "After 10 iterations:\n",
      "0.6545815404878357\n",
      "After 50 iterations:\n",
      "0.5410281654060278\n",
      "After 100 iterations:\n",
      "0.4646355456241524\n",
      "Learning Rate: 0.1\n",
      "#######################################\n",
      "After 10 iterations:\n",
      "0.47180555287857295\n",
      "After 50 iterations:\n",
      "0.30769369524848067\n",
      "After 100 iterations:\n",
      "0.2657359811313242\n",
      "Learning Rate: 1\n",
      "#######################################\n",
      "After 10 iterations:\n",
      "0.26529992503938243\n",
      "After 50 iterations:\n",
      "0.21396861072354953\n",
      "After 100 iterations:\n",
      "0.20232597927385523\n"
     ]
    }
   ],
   "source": [
    "for Learning_rate in [0.01,0.1,1]:\n",
    "    print(\"Learning Rate:\", Learning_rate)\n",
    "    print(\"#######################################\")\n",
    "    for iterations in [10, 50, 100]:\n",
    "        print('After {} iterations:'.format(iterations))\n",
    "        GD(X_test,Y_test,theta, Learning_rate,iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 iterrations\n",
      "For Learning Rate 0.01\n",
      "Loss:\n",
      "0.4646355456241524\n",
      "Accuracy 0.6194613379669852\n",
      "F1 Score 0.05194805194805195\n",
      "For Learning Rate 0.1\n",
      "Loss:\n",
      "0.2657359811313242\n",
      "Accuracy 0.792354474370113\n",
      "F1 Score 0.6459259259259259\n",
      "For Learning Rate 0.5\n",
      "Loss:\n",
      "0.21406245963556694\n",
      "Accuracy 0.845351867940921\n",
      "F1 Score 0.7588075880758808\n"
     ]
    }
   ],
   "source": [
    "print(\"After 100 iterrations\")\n",
    "for Learning_rate in [0.01,0.1,0.5]:\n",
    "    print(\"For Learning Rate\",Learning_rate )\n",
    "    print(\"Loss:\")\n",
    "    y_pred = GD(X_test,Y_test,theta, Learning_rate,100)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred = y_pred.applymap(lambda x: 1 if x>i else 0)\n",
    "    print(\"Accuracy\", accuracy_score(Y_test, y_pred))\n",
    "    print(\"F1 Score\", f1_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurary is 86%. More iterations or a higher learning rate is needed to reach package results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lgr = linear_model.LogisticRegression()\n",
    "\n",
    "##Train Data\n",
    "lgr.fit(X_train,Y_train)\n",
    "y_train_log = lgr.predict(X_train)\n",
    "\n",
    "##Test Data\n",
    "lgr.fit(X_test,Y_test)\n",
    "y_test_log = lgr.predict(X_test)\n",
    "\n",
    "\n",
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "##Train Dta\n",
    "X_lda =lda.fit_transform(X_train,Y_train)\n",
    "y_train_lda = lda.predict(X_train)\n",
    "\n",
    "##Test Data\n",
    "X_lda =lda.fit_transform(X_test,Y_test)\n",
    "y_test_lda = lda.predict(X_test)\n",
    "\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "##Train Data\n",
    "knn1.fit(X_train,Y_train)\n",
    "y_train_knn1 = knn1.predict(X_train)\n",
    "\n",
    "\n",
    "##Test Data\n",
    "knn1.fit(X_test,Y_test)\n",
    "y_test_knn1 = knn1.predict(X_test)\n",
    "\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "\n",
    "#Train Data\n",
    "NB.fit(X_train, Y_train)\n",
    "y_train_nb = NB.predict(X_train)\n",
    "\n",
    "#Test Data\n",
    "NB.fit(X_test, Y_test)\n",
    "y_test_nb = NB.predict(X_test)\n",
    "\n",
    "\n",
    "#Decesion Trees\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "#Train Data\n",
    "dt.fit(X_train, Y_train)\n",
    "y_train_dt = dt.predict(X_train)\n",
    "\n",
    "#Test Data\n",
    "dt.fit(X_test, Y_test)\n",
    "y_test_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 1\n",
      "KNN Train Accuracy 0.9997101449275362\n",
      "KNN Train Error 0.00028985507246381825\n",
      "KNN Test Accuracy 1.0\n",
      "KNN Test Error 0.0\n",
      "For K = 2\n",
      "KNN Train Accuracy 0.9515942028985507\n",
      "KNN Train Error 0.04840579710144932\n",
      "KNN Test Accuracy 0.945264986967854\n",
      "KNN Test Error 0.05473501303214601\n",
      "For K = 3\n",
      "KNN Train Accuracy 0.9426086956521739\n",
      "KNN Train Error 0.05739130434782613\n",
      "KNN Test Accuracy 0.9417897480451781\n",
      "KNN Test Error 0.05821025195482188\n",
      "For K = 4\n",
      "KNN Train Accuracy 0.931304347826087\n",
      "KNN Train Error 0.06869565217391305\n",
      "KNN Test Accuracy 0.9226759339704604\n",
      "KNN Test Error 0.07732406602953956\n",
      "For K = 5\n",
      "KNN Train Accuracy 0.9330434782608695\n",
      "KNN Train Error 0.06695652173913047\n",
      "KNN Test Accuracy 0.9252823631624674\n",
      "KNN Test Error 0.07471763683753263\n",
      "For K = 6\n",
      "KNN Train Accuracy 0.9223188405797101\n",
      "KNN Train Error 0.07768115942028986\n",
      "KNN Test Accuracy 0.9113814074717637\n",
      "KNN Test Error 0.08861859252823634\n",
      "For K = 7\n",
      "KNN Train Accuracy 0.923768115942029\n",
      "KNN Train Error 0.07623188405797099\n",
      "KNN Test Accuracy 0.9218071242397915\n",
      "KNN Test Error 0.0781928757602085\n",
      "For K = 8\n",
      "KNN Train Accuracy 0.9153623188405797\n",
      "KNN Train Error 0.08463768115942027\n",
      "KNN Test Accuracy 0.9087749782797567\n",
      "KNN Test Error 0.09122502172024327\n",
      "For K = 9\n",
      "KNN Train Accuracy 0.9185507246376812\n",
      "KNN Train Error 0.08144927536231883\n",
      "KNN Test Accuracy 0.9096437880104257\n",
      "KNN Test Error 0.09035621198957433\n",
      "For K = 10\n",
      "KNN Train Accuracy 0.9110144927536232\n",
      "KNN Train Error 0.08898550724637677\n",
      "KNN Test Accuracy 0.895742832319722\n",
      "KNN Test Error 0.10425716768027804\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,11):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    print(\"For K =\", k)\n",
    "    ##Train Data\n",
    "    knn.fit(X_train,Y_train)\n",
    "    y_train_knn = knn.predict(X_train)\n",
    "    print(\"KNN Train Accuracy\", accuracy_score(Y_train, y_train_knn))\n",
    "    print(\"KNN Train Error\", 1 - accuracy_score(Y_train, y_train_knn))\n",
    "    ##Test Data\n",
    "    knn.fit(X_test,Y_test)\n",
    "    y_test_knn = knn.predict(X_test)\n",
    "    print(\"KNN Test Accuracy\", accuracy_score(Y_test, y_test_knn))\n",
    "    print(\"KNN Test Error\", 1- accuracy_score(Y_test, y_test_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Train Accuracy 0.9295652173913044\n",
      "Test Accuracy 0.944396177237185\n",
      "Train Error 0.07043478260869562\n",
      "Test Error 0.05560382276281495\n",
      "LDA\n",
      "Train Accuracy 0.8846376811594203\n",
      "Test Accuracy 0.9070373588184187\n",
      "Train Error 0.11536231884057968\n",
      "Test Error 0.09296264118158126\n",
      "KNN\n",
      "Train Accuracy 0.9997101449275362\n",
      "Test Accuracy 1.0\n",
      "Train Error 0.00028985507246381825\n",
      "Test Error 0.0\n",
      "Naive Bayes\n",
      "Train Accuracy 0.8182608695652174\n",
      "Test Accuracy 0.8218940052128584\n",
      "Train Error 0.18173913043478263\n",
      "Test Error 0.17810599478714162\n",
      "Decesion Tree\n",
      "Train Accuracy 0.9997101449275362\n",
      "Test Accuracy 1.0\n",
      "Train Error 0.00028985507246381825\n",
      "Test Error 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Train Accuracy\", accuracy_score(Y_train, y_train_log))\n",
    "print(\"Test Accuracy\", accuracy_score(Y_test, y_test_log))\n",
    "print(\"Train Error\", 1- accuracy_score(Y_train, y_train_log))\n",
    "print(\"Test Error\",1-  accuracy_score(Y_test, y_test_log))\n",
    "\n",
    "print(\"LDA\")\n",
    "print(\"Train Accuracy\", accuracy_score(Y_train, y_train_lda))\n",
    "print(\"Test Accuracy\", accuracy_score(Y_test, y_test_lda))\n",
    "print(\"Train Error\", 1- accuracy_score(Y_train, y_train_lda))\n",
    "print(\"Test Error\",1-  accuracy_score(Y_test, y_test_lda))\n",
    "\n",
    "print(\"KNN\")\n",
    "print(\"Train Accuracy\", accuracy_score(Y_train, y_train_knn1))\n",
    "print(\"Test Accuracy\", accuracy_score(Y_test, y_test_knn1))\n",
    "print(\"Train Error\", 1- accuracy_score(Y_train, y_train_knn1))\n",
    "print(\"Test Error\",1-  accuracy_score(Y_test, y_test_knn1))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(\"Train Accuracy\", accuracy_score(Y_train, y_train_nb))\n",
    "print(\"Test Accuracy\", accuracy_score(Y_test, y_test_nb))\n",
    "print(\"Train Error\", 1- accuracy_score(Y_train, y_train_nb))\n",
    "print(\"Test Error\",1-  accuracy_score(Y_test, y_test_nb))\n",
    "\n",
    "print(\"Decesion Tree\")\n",
    "print(\"Train Accuracy\", accuracy_score(Y_train, y_train_dt))\n",
    "print(\"Test Accuracy\", accuracy_score(Y_test, y_test_dt))\n",
    "print(\"Train Error\", 1- accuracy_score(Y_train, y_train_dt))\n",
    "print(\"Test Error\",1-  accuracy_score(Y_test, y_test_dt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN and Decesion Tree has high Test Accuracy. Logistic regression is performing best. Naive bayes is worst comparetively.Knn test accuracy is perfect because of k value. Decesion Tree also produces perfect classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC FOR:\n",
      "Logistic Regression: 0.9785259153590109\n",
      "LDA: 0.9616072277698526\n",
      "KNN: 1.0\n",
      "NB: 0.9517213504517357\n",
      "Decesion Tree: 1.0\n"
     ]
    }
   ],
   "source": [
    "log_score = lgr.predict_proba(X_test)\n",
    "lda_score = lda.predict_proba(X_test)\n",
    "knn_score = knn1.predict_proba(X_test)\n",
    "nb_score = NB.predict_proba(X_test)\n",
    "dt_score = dt.predict_proba(X_test)\n",
    "#print(dt_score)\n",
    "\n",
    "fpr_log, tpr_log, t = roc_curve(Y_test, log_score[:,1])\n",
    "fpr_lda, tpr_lda, t = roc_curve(Y_test, lda_score[:,1])\n",
    "fpr_knn, tpr_knn, t = roc_curve(Y_test, knn_score[:,1])\n",
    "fpr_nb, tpr_nb, t = roc_curve(Y_test, nb_score[:,1])\n",
    "fpr_dt, tpr_dt, t = roc_curve(Y_test, dt_score[:,1])\n",
    "\n",
    "plt.plot(fpr_log,tpr_log, label = 'log')\n",
    "plt.plot(fpr_lda,tpr_lda, label = 'lda')\n",
    "plt.plot(fpr_knn, tpr_knn, label = 'knn')\n",
    "plt.plot(fpr_nb, tpr_nb, label = 'nb')\n",
    "plt.plot(fpr_dt, tpr_dt, label = 'dt')\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.legend()\n",
    "plt.title(\"ROC CURVE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"AUC FOR:\")\n",
    "print(\"Logistic Regression:\", roc_auc_score(Y_test, log_score[:,1]))\n",
    "print(\"LDA:\", roc_auc_score(Y_test, lda_score[:,1]))\n",
    "print(\"KNN:\", roc_auc_score(Y_test, knn_score[:,1]))\n",
    "print(\"NB:\", roc_auc_score(Y_test, nb_score[:,1]))\n",
    "print(\"Decesion Tree:\", roc_auc_score(Y_test, dt_score[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.sample(frac = 1)\n",
    "#train, test = data.iloc[:100,:], data.iloc[100:200,:]\n",
    "def getData(file_path):\n",
    "    spam_input = []\n",
    "    ham_input = []\n",
    "    labels = []\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            labels.append(row[57])\n",
    "            if(row[57]=='1'):\n",
    "                spam_input.append(row)\n",
    "            else:\n",
    "                ham_input.append(row)\n",
    "\n",
    "\n",
    "    percent_spam = int((labels.count('1')*100)/len(labels))\n",
    "    return spam_input,ham_input,percent_spam\n",
    "\n",
    "def getTrainTestData(spam_input,ham_input,percent_spam):\n",
    "    spam_sample = random.sample(spam_input,2*percent_spam)\n",
    "    ham_sample = random.sample(ham_input,2*(100-percent_spam))\n",
    "    train_data = spam_sample[:percent_spam]+ham_sample[:(100-percent_spam)]\n",
    "    test_data = spam_sample[percent_spam:]+ham_sample[(100-percent_spam):]\n",
    "    return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEuclideanDistance(x,y):\n",
    "    distance = 0\n",
    "    for i in range(57):\n",
    "        distance += (float(x[i])-float(y[i]))**2\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_data,test_data,k):\n",
    "    label_set = list()\n",
    "    for test_row in test_data:\n",
    "        distances = list()\n",
    "        for train_row in train_data:\n",
    "            dist = getEuclideanDistance(test_row,train_row)\n",
    "            distances.append((train_row[-1],dist))\n",
    "        distances.sort(key=lambda tup: tup[1])\n",
    "        neighbors = distances[:k]\n",
    "        output_values = [row[0][-1] for row in neighbors]\n",
    "        prediction = max(set(output_values), key=output_values.count)\n",
    "        label_set.append((prediction,test_row[57]))\n",
    "    return label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAccuracyError(labels):\n",
    "    right =0\n",
    "    wrong =0\n",
    "    for each in labels:\n",
    "        if(each[0]==each[1]):\n",
    "            right+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "    accuracy = right/(right+wrong)\n",
    "    error = wrong/(right+wrong)\n",
    "    return accuracy,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for a test point: ['0', '0.89', '1.14', '0', '0.12', '0', '0', '0.12', '0.25', '0.12', '0.12', '0.38', '0.12', '0', '1.14', '0', '0.12', '2.04', '2.8', '0.12', '1.27', '0', '0', '0.12', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0.12', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0.165', '0', '0.371', '0.061', '0', '2.878', '84', '475', '1'] = 1\n"
     ]
    }
   ],
   "source": [
    "file_path  =  \"spambase.data\"\n",
    "spam_input,ham_input,percent_spam = getData(file_path)\n",
    "\n",
    "train_data,test_data = getTrainTestData(spam_input,ham_input,percent_spam)\n",
    "#print(test_data)\n",
    "final_labels = KNN(train_data,[test_data[0]],k=3)\n",
    "\n",
    "print(\"Prediction for a test point:\",test_data[0],  \"=\", final_labels[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1\n",
      "Accuracy: 1.0\n",
      "Error: 0.0\n",
      "For k =  2\n",
      "Accuracy: 0.8\n",
      "Error: 0.2\n",
      "For k =  3\n",
      "Accuracy: 0.82\n",
      "Error: 0.18\n",
      "For k =  4\n",
      "Accuracy: 0.77\n",
      "Error: 0.23\n",
      "For k =  5\n",
      "Accuracy: 0.79\n",
      "Error: 0.21\n",
      "For k =  6\n",
      "Accuracy: 0.77\n",
      "Error: 0.23\n",
      "For k =  7\n",
      "Accuracy: 0.8\n",
      "Error: 0.2\n",
      "For k =  8\n",
      "Accuracy: 0.77\n",
      "Error: 0.23\n",
      "For k =  9\n",
      "Accuracy: 0.74\n",
      "Error: 0.26\n",
      "For k =  10\n",
      "Accuracy: 0.79\n",
      "Error: 0.21\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,11):\n",
    "    final_labels = KNN(test_data,test_data,k)\n",
    "\n",
    "    accuracy,error = getAccuracyError(final_labels)\n",
    "    print(\"For k = \", k)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Error:\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy with k = 1 is same. With other k values it seems to differ quite a bit. Perhaps because of limited data in each fold as k increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for k =  1 \n",
      " 1.1224675000012212\n",
      "Time for k =  2 \n",
      " 1.172322800000984\n",
      "Time for k =  3 \n",
      " 1.135630200002197\n",
      "Time for k =  4 \n",
      " 1.22042220000003\n",
      "Time for k =  5 \n",
      " 1.1021535999971093\n",
      "Time for k =  6 \n",
      " 1.1666210000003048\n",
      "Time for k =  7 \n",
      " 1.1203548999983468\n",
      "Time for k =  8 \n",
      " 1.1009221999993315\n",
      "Time for k =  9 \n",
      " 1.1389304999975138\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "for k in range(1,10):\n",
    "    start = timeit.default_timer()\n",
    "    final_labels = KNN(test_data,test_data,k)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time for k = ',k, '\\n',stop - start)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24820216287594976\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "folds = np.array_split(data, k)\n",
    "error = []\n",
    "for i in range(k):\n",
    "        train = folds.copy() \n",
    "        test = folds[i]\n",
    "        testx = test.iloc[:,:57]\n",
    "        testy = test[\"if_spam\"]\n",
    "        del train[i]\n",
    "        train = pd.concat(train,sort=False)\n",
    "        trainx = train.iloc[:,:57]\n",
    "        trainy = train[\"if_spam\"]\n",
    "        #FITTING LGR\n",
    "        lgr.fit(trainx,trainy)\n",
    "        y_pred = lgr.predict(testx)\n",
    "        validation_error = 1 - accuracy_score(testy, y_pred)\n",
    "        error.append(validation_error)\n",
    "print(np.mean(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  5\n",
      "Logistic average validation error 0.14495633290846432\n",
      "LDA average validation error 0.1838320823301704\n",
      "For k =  10\n",
      "Logistic average validation error 0.10670187682731305\n",
      "LDA average validation error 0.1479661416580213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in [5, 10]:\n",
    "    print(\"For k = \",k)\n",
    "    folds = np.array_split(data, k)\n",
    "    \n",
    "    lgr_error_list = []\n",
    "    lda_error_list = []\n",
    "    for i in range(k):\n",
    "        train = folds.copy() \n",
    "        test = folds[i]\n",
    "        testx = test.iloc[:,:57]\n",
    "        testy = test[\"if_spam\"]\n",
    "        del train[i]\n",
    "        train = pd.concat(train,sort=False)\n",
    "        trainx = train.iloc[:,:57]\n",
    "        trainy = train[\"if_spam\"]\n",
    "        #FITTING LGR\n",
    "        lgr.fit(trainx,trainy)\n",
    "        y_pred = lgr.predict(testx)\n",
    "        validation_error = 1 - accuracy_score(testy, y_pred)\n",
    "        lgr_error_list.append(validation_error)\n",
    "        #FITTING LDA\n",
    "        lda.fit(trainx,trainy)\n",
    "        y_pred = lda.predict(testx)\n",
    "        validation_error = 1 - accuracy_score(testy, y_pred)\n",
    "        lda_error_list.append(validation_error)\n",
    "    print(\"Logistic average validation error\", np.mean(lgr_error_list))\n",
    "    print(\"LDA average validation error\", np.mean(lda_error_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regrssion performs better.\n",
    "LDA has higher validation error than logistic regression  in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
